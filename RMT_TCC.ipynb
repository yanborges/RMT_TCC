{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2cfVKzLtIRg1"
   },
   "outputs": [],
   "source": [
    "#Importando as bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#from io import StringIO, BytesIO\n",
    "#from urllib.request import urlopen\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bhv4Z-9sZQgw"
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers , models\n",
    "from keras.layers import Flatten, Dense, Dropout, Input\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhU0QgcMSZyD"
   },
   "source": [
    "## A fazer\n",
    "\n",
    "Fazer um código que funcione para todos os datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xunbDQxaSAan"
   },
   "source": [
    "## Duvidas\n",
    "\n",
    "\n",
    "1.   Não é necessário normalizar os dados antes?\n",
    "2.   É preciso fazer algum tipo de vizualizaçao de dados ou de resultados\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUh8kiCjVxO0"
   },
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "byiB0yYdVwHx"
   },
   "outputs": [],
   "source": [
    "#DecisionTree\n",
    "def model_dtree(x_train,y_train,x_test,y_test,booster,eta,max_depth,colsample_bytree,subsample,min_child_weight,reg_lambda,num_boost,my_num):\n",
    "    bst = DecisionTreeRegressor().fit(x_train, y_train)\n",
    "    predict = bst.predict(x_test)\n",
    "    return predict\n",
    "\n",
    "#RandomForestRegressor\n",
    "def model_r(x_train,y_train,x_test,y_test,my_num):\n",
    "    #booster,eta,max_depth,colsample_bytree,subsample,min_child_weight,reg_lambda,num_boost):\n",
    "    bst = RandomForestRegressor(n_estimators = my_num,n_jobs = -1).fit(x_train,y_train)\n",
    "    predict = bst.predict(x_test)\n",
    "    return predict\n",
    "\n",
    "#SVR\n",
    "def model_svr(x_train,y_train,x_test,y_test,ker,gam,c,epsi,shrink,verb,max_it):\n",
    "    bst = SVR(kernel=ker, gamma=gam, C=c, epsilon=epsi, shrinking=shrink, verbose=verb, max_iter=max_it).fit(x_train,y_train)\n",
    "    predict = bst.predict(x_test)\n",
    "    return predict\n",
    "\n",
    "def model_multi_dtree(x_train, y_train, x_test, y_test):\n",
    "    model = DecisionTreeRegressor()\n",
    "    bst = MultiOutputRegressor(model).fit(x_train, y_train)\n",
    "    predict = bst.predict(x_test)\n",
    "    return predict\n",
    "\n",
    "def model_ann(x_train, y_train, x_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (data.shape[1])))\n",
    "    model.add(Dense(64, input_dim=4, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0, validation_split=0.2)\n",
    "    predict = model.predict(x_test)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de erro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_RRMSE_list(real_test,result_p,real_train_mean):\n",
    "    \"\"\"RRMESE: Relative Root Mean Square Error\n",
    "        input: real_test: real test data\n",
    "              result_p: predicted data\n",
    "              real_train_mean: mean of real train data\n",
    "        output: RRMSE list \"\"\"\n",
    "\n",
    "    _list = []\n",
    "    for i in range(result_p.shape[1]):\n",
    "        fenzi = 0\n",
    "        fenmu = 0\n",
    "        for j in range(result_p.shape[0]):\n",
    "            fenzi += (result_p[j,i] - real_test[j,i])**2\n",
    "            fenmu += (real_train_mean[i] - real_test[j,i])**2\n",
    "        _list.append(math.sqrt(fenzi/fenmu))    \n",
    "    return _list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unpvRVKQSTwF"
   },
   "source": [
    "## Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ohf0EgDuYRXL",
    "outputId": "04f5473d-f761-4005-d640-dae2cb96522e"
   },
   "outputs": [],
   "source": [
    "path = \"../TCC/mtr-datasets/\"\n",
    "\n",
    "#text_file = input(\"dataset: \")\n",
    "text_file = 'atp1d'\n",
    "data, meta = loadarff(path + text_file + \".arff\")\n",
    "data = pd.DataFrame(data)\n",
    "data.reset_index(inplace=True)\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.replace('     ?', np.nan, inplace=True)\n",
    "data = data.applymap(float)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1mux8LFCbKwV"
   },
   "outputs": [],
   "source": [
    "if config.all_config[text_file].get('sample_random') == True:\n",
    "    data = data.sample(frac=1,random_state = config.all_config[text_file].get('sample_random_num')).reset_index(drop=True)\n",
    "data = data.fillna(pd.Series(np.nanmean(data,axis=0),index=data.columns))\n",
    "label = data.iloc[:,-config.all_config[text_file].get('targets_num'):].values\n",
    "data = data.iloc[:,:-config.all_config[text_file].get('targets_num')].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRMSE:  0.43056804336541504\n"
     ]
    }
   ],
   "source": [
    "# Teste Multi-dtree\n",
    "loss_list = [] \n",
    "kf = KFold(n_splits=config.all_config[text_file].get('paper_kFold'), shuffle=config.all_config[text_file].get('kfold_random'))\n",
    "for train_index , test_index in kf.split(data):\n",
    "    x_train, x_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    result_p_train = []\n",
    "    result_p_test = []\n",
    "    r1 = model_multi_dtree(x_train, y_train, x_test, y_test)\n",
    "    loss_list = computer_RRMSE_list(y_test,r1,np.mean(y_train,axis=0))\n",
    "    \n",
    "print(\"RRMSE: \", np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRMSE:  [1.94205537]\n",
      "1.942055367322777\n"
     ]
    }
   ],
   "source": [
    "loss_list_RRMSE = []\n",
    "kf = KFold(n_splits=config.all_config[text_file].get('paper_kFold'),shuffle=config.all_config[text_file].get('kfold_random'),random_state=config.all_config[text_file].get('kfold_random_num'))\n",
    "for train_index, test_index in kf.split(data):\n",
    "    x_train, x_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    result_p_train = []\n",
    "    result_p_test = []\n",
    "    r1_list = []\n",
    "    r1 = None \n",
    "    for i in range(label.shape[1]):\n",
    "        if text_file in ['sf1', 'sf2']:\n",
    "            r1 = model_svr(x_train,y_train[:,i],x_test,y_test[:,i],\n",
    "                            config.all_config[text_file].get('svr1_kernel'),\n",
    "                            config.all_config[text_file].get('svr1_gamma'),\n",
    "                            config.all_config[text_file].get('svr1_C'),\n",
    "                            config.all_config[text_file].get('svr1_epsilon'),\n",
    "                            config.all_config[text_file].get('svr1_shrinking'),\n",
    "                            config.all_config[text_file].get('svr1_verbose'),\n",
    "                            config.all_config[text_file].get('svr1_max_iter'))\n",
    "                        #    'rbf',0.1,100,0.8,True,True,False,10000)\n",
    "        else:\n",
    "            r1 = model_r(x_train,y_train[:,i],x_test,y_test[:,i], my_num=100)\n",
    "        r1_list.append(r1)\n",
    "    result_p_train.append(np.concatenate(r1_list,axis=0))\n",
    "    predicted_test = None \n",
    "    if text_file in ['sf1', 'sf2']:\n",
    "        predicted_test = model_svr(x_train, y_train[:,i], x_test, y_test[:,i],\n",
    "                                   config.all_config[text_file].get('svr1_kernel'),\n",
    "                                   config.all_config[text_file].get('svr1_gamma'),\n",
    "                                   config.all_config[text_file].get('svr1_C'),\n",
    "                                   config.all_config[text_file].get('svr1_epsilon'),\n",
    "                                   config.all_config[text_file].get('svr1_shrinking'),\n",
    "                                   config.all_config[text_file].get('svr1_verbose'),\n",
    "                                   config.all_config[text_file].get('svr1_max_iter'))\n",
    "    else:\n",
    "        predicted_test = model_r(x_train, y_train[:,i], x_test, y_test[:,i], my_num=100)\n",
    "    result_p_test.append(predicted_test)\n",
    "result_p_train = pd.DataFrame(result_p_train).T.values\n",
    "result_p_test = pd.DataFrame(result_p_test).T.values\n",
    "real_train_mean = y_train.mean(axis=0)\n",
    "loss_list_RRMSE.append(computer_RRMSE_list(y_test,result_p_test,real_train_mean))\n",
    "loss_list_RRMSE_np = np.array(loss_list_RRMSE)\n",
    "\n",
    "print('RRMSE: ',loss_list_RRMSE_np.mean(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 14:47:55.817479: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x296704dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x295b32440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         r1_list\u001b[39m.\u001b[39mappend(r1)\n\u001b[1;32m     17\u001b[0m     result_p_train\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mconcatenate(r1_list,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m     loss_list \u001b[39m=\u001b[39m computer_RRMSE_list(y_test,result_p_train,np\u001b[39m.\u001b[39;49mmean(y_train,axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRRMSE: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(loss_list))\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mcomputer_RRMSE_list\u001b[0;34m(real_test, result_p, real_train_mean)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\"\"\"RRMESE: Relative Root Mean Square Error\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    input: real_test: real test data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m          result_p: predicted data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m          real_train_mean: mean of real train data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m    output: RRMSE list \"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m _list \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(result_p\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     10\u001b[0m     fenzi \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     11\u001b[0m     fenmu \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Teste ANN\n",
    "# Ainda não está pronto\n",
    "loss_list = [] \n",
    "kf = KFold(n_splits=config.all_config[text_file].get('paper_kFold'), shuffle=config.all_config[text_file].get('kfold_random'))\n",
    "for train_index , test_index in kf.split(data):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "  \n",
    "    result_p_train = []\n",
    "    result_p_test = []\n",
    "    r1_list = []\n",
    "    \n",
    "    for i in range(label.shape[1]):\n",
    "        r1 = model_ann(x_train, y_train, x_test, y_test)\n",
    "        r1_list.append(r1)\n",
    "    result_p_train.append(np.concatenate(r1_list,axis=0))\n",
    "\n",
    "    loss_list = computer_RRMSE_list(y_test,result_p_train,np.mean(y_train,axis=0))\n",
    "    \n",
    "print(\"RRMSE: \", np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmj77EwpZ9DS"
   },
   "source": [
    "### Carregando Datasets (desatualizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52IrBAApYiHd"
   },
   "outputs": [],
   "source": [
    "# path = '/content/drive/MyDrive/TCC/mtr-datasets/'\n",
    "\n",
    "# andro = path + 'andro.arff'\n",
    "# atp1d = path + 'atp1d.arff'\n",
    "# atp7d = path + 'atp7d.arff'\n",
    "# edm = path + 'edm.arff'\n",
    "# enb = path + 'enb.arff'\n",
    "# jura = path + 'jura.arff'\n",
    "# oes10 = path + 'oes10.arff'\n",
    "# oes97 = path + 'oes97.arff'\n",
    "# osales = path + 'osales.arff'\n",
    "# rf1 = path + 'rf1.arff'\n",
    "# rf2 = path + 'rf2.arff'\n",
    "# scm1d = path + 'scm1d.arff'\n",
    "# scm20d = path + 'scm20d.arff'\n",
    "# scpf = path + 'scpf.arff'\n",
    "# sf1 = path + 'sf1.arff'\n",
    "# sf2 = path + 'sf2.arff'\n",
    "# slump = path + 'slump.arff'\n",
    "# wq = path + 'wq.arff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXCzdS32bcX-"
   },
   "outputs": [],
   "source": [
    "# data_andro, meta_andro = loadarff(andro)\n",
    "# df_andro = pd.DataFrame(data_andro)\n",
    "\n",
    "# data_atp1d, meta_atp1d = loadarff(atp1d)\n",
    "# df_atp1d = pd.DataFrame(data_atp1d)\n",
    "\n",
    "# data_atp7d, meta_atp7d = loadarff(atp7d)\n",
    "# df_atp7d = pd.DataFrame(data_atp7d)\n",
    "\n",
    "# data_edm, meta_edm = loadarff(edm)\n",
    "# df_edm = pd.DataFrame(data_edm)\n",
    "\n",
    "# data_enb, meta_enb = loadarff(enb)\n",
    "# df_enb = pd.DataFrame(data_enb)\n",
    "\n",
    "# data_jura, meta_jura = loadarff(jura)\n",
    "# df_jura = pd.DataFrame(data_jura)\n",
    "\n",
    "# data_jura, meta_jura = loadarff(jura)\n",
    "# df_jura = pd.DataFrame(data_jura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz7HnC3rgBOo"
   },
   "outputs": [],
   "source": [
    "# data_oes10, meta_oes10 = loadarff(oes10)\n",
    "# df_oes10 = pd.DataFrame(data_oes10)\n",
    "\n",
    "# data_oes97, meta_oes97 = loadarff(oes97)\n",
    "# df_oes97 = pd.DataFrame(data_oes97)\n",
    "\n",
    "# data_osales, meta_osales = loadarff(osales)\n",
    "# df_osales = pd.DataFrame(data_osales)\n",
    "\n",
    "# data_rf1, meta_rf1 = loadarff(rf1)\n",
    "# df_rf1 = pd.DataFrame(data_rf1)\n",
    "\n",
    "# data_rf2, meta_rf2 = loadarff(rf2)\n",
    "# df_rf2 = pd.DataFrame(data_rf2)\n",
    "\n",
    "# data_scm1d, meta_scm1d = loadarff(scm1d)\n",
    "# df_scm1d = pd.DataFrame(data_scm1d)\n",
    "\n",
    "# data_scm20d, meta_scm20d = loadarff(scm20d)\n",
    "# df_scm20d = pd.DataFrame(data_scm20d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB8wfIW9ga_E"
   },
   "outputs": [],
   "source": [
    "# data_scpf, meta_scpf = loadarff(scpf)\n",
    "# df_scpf = pd.DataFrame(data_scpf)\n",
    "\n",
    "# data_sf1, meta_sf1 = loadarff(sf1)\n",
    "# df_sf1 = pd.DataFrame(data_sf1)\n",
    "\n",
    "# data_sf2, meta_sf2 = loadarff(sf2)\n",
    "# df_sf2 = pd.DataFrame(data_sf2)\n",
    "\n",
    "# data_slump, meta_slump = loadarff(slump)\n",
    "# df_slump = pd.DataFrame(data_slump)\n",
    "\n",
    "# data_wq, meta_wq = loadarff(wq)\n",
    "# df_wq = pd.DataFrame(data_wq)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
